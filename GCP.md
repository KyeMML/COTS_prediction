# GCP
Walkthrough of steps taken to host model on GCP. A variety of these services may require you to enable their API on GCP. You should be prompted to do this when required.  


### Kaggle dataset
The dataset includes a folder labelled train_images, containing images of COTS, as well as a csv file labelled train.csv, containing the annotations of these images. 
The annotations include labelled object detection boxes for COTS manually identified in the images.


## 1. Preprocessing
The train.csv file providing annotated labels for COTS dataset is not initially in acceptable format for GCP Vertex AI object detection.  
The transformation of this data can be completed as ETL or ELT and is discussed further in 2.3 Dataflow.

#### Transforming the training data CSV for object detection  
More information on this is provided by google [here](https://cloud.google.com/vision/automl/object-detection/docs/csv-format).  
The CSV file must comply with the following:
- CSV file and the images it points to must be within the same GCP cloud storage bucket
- The csv file must be UTF-8 encoded, .csv extension
- File has one row for each bounding box or one row for each image with no bounding box. 
- The file must contain one image per line. Images with multiple bounding boxes will be repeated with a unique bounding box coordinates on each row.  
An example is:  
```
TRAIN, gs://folder/image1.png,car,0.1,0.1,,,0.3,0.3,,
TRAIN,gs://folder/image1.png,bike,.7,.6,,,.8,.9,,
UNASSIGNED,gs://folder/im2.png,car,0.1,0.1,0.2,0.1,0.2,0.3,0.1,0.3
TEST,gs://folder/im3.png,,,,,,,,,
```
Where image1 is repeated for the bounding box of a car, and again for the bounding box of a bike on the following line.  
The necessary columns include:  
- assignment (train/test/validation/unassigned)
- URI (points to image location on cloud storage bucket)
- label (in our case it would be COTS)
- bounding box (can be specified with two vertices or all 4 vertices). Each vertex is defined by x and y coordinates. **These coordinates must be a float and min max normalised across 0-1 range**.

Assignment can be achieved manually by splitting the data, or letting GCP do this within the GCP Vertex AI console.  
URI can be generated by iterating across image id into a f-string. For example:  
```python
for image_number in dataset.video_frame:
  GCP_BUCKET = f'gs://reef_data/cots_images/{image_number}.jpeg'
```  
Bounding boxes in the raw data are lists of dictionaries containing an height, width, x and y coordinates. This will need to be translated into the required format.  

## 2. Cloud Storage
To upload the Kaggle image data and train data CSV into GCP Cloud storage, we first create a new bucket.   
To do this navigate to the GCP Cloud Storage dashboard in GCP console.  
Click Create Bucket  
The bucket name and location is up to the user. It should be noted that regions must be the same across Vertex AI and cloud storage services. US regions are recommended.  
Standard storage class is recommended for this project.  
Access controls and protecting data options is up to the user.  

After this, a new bucket should be created. Here you can now upload the data. To do this, navigate into the bucket, and click upload files or upload folder.


### 2.2 BigQuery
If necessary, to query the available data in cloud storage:
- create a new dataset in your project ID in the BigQuery dashboard.
- within that new dataset, create a table. 
  - this will provide a range of options including specifying a source. Here specify cloud storage, and then the specific file contained in the cloud storage bucket.

### 2.3 Data Pipelines
if necessary, you can use dataflow for batch processing, or dataproc for streaming. Basstch processing in dataflow is recommended for ELT, more information on Dataflow can be found [here](https://github.com/KyeMML/GCP/blob/main/Batch_Data_Pipelines/Dataflow.md).  
At a high level, raw data ingested into cloud storage would feed into the dataflow to facilitate the preprocessing discussed earlier. It should be noted that automating these pipelines would facilitate a trade off between manual jobs and financial expenses generating by greater resource consumption on GCP. To determine what is best between ETL and ELT you can find more information [here](https://github.com/KyeMML/GCP/blob/main/Data_Lakes_Data_Wharehouses/Data_Lakes.md).

## 3. Vertex AI
To utilise the Vertex AI platform, navigate to Vertex AI dashboard on google console.  
There are three options available; Create Dataset, Train new Model, Create batch predictions.  
For this project, select Create Dataset.  
The dataset name is up to the user.    
Select image object detection.  
Select a region (must match the region of cloud storage bucket containing the data).    
Click create.  

To import the data, select "import files from Cloud Storage". (This is because the dataset contains labels)  
Next, provide the import file path that points to the train data CSV file discussed above. It is necessary for this file to be in accepted format as discussed above.  
Click continue  
You will be notified by email when this process is completed.  
This dataset can now be utilised to train a new model.  

### 3.1 Model Training  
Vertex AI provides the following model options:  
![image](https://user-images.githubusercontent.com/55074122/154063281-e661ee1f-ff35-416c-813c-2ad1c34963df.png)

#### AutoML
**Model detail options include:**  
- Model name
- Data split
- encryptions
**Training Options include:**  
1. Higher Accuracy (expected higher accuracy, latency 800ms-1,500ms)
2. Faster predictions (expected lower accuracy, Latency 300ms-500ms)
**Compute and Pricing:**  
You must provide the maximum number of node hours to spend training the model. The minimum is 20 hours.
